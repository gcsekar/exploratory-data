---
title: "README"
author: "Chandrasekar Ganesan"
date: "March 2, 2017"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Hierarchical Clustering - Example
```{r}
set.seed(1234)
par(mar=c(0,0,0,0))
x <- rnorm(12, mean=rep(1:3, each=4), sd=0.2)
y <- rnorm(12, mean=rep(1,2,1, each=4), sd=0.2)
plot(x, y, col='blue', pch=19, cex=2)
text(x + 0.05, y + 0.05, labels(as.character(1:12)))
```

>Hierarchical Clusterring - Dist
```{r}
dataFrame <- data.frame(x = x, y = y)
dist(dataFrame)
```

>Hierarchical Clustering- hclust
```{r}
distxy <- dist(dataFrame)
hClustering <- hclust(distxy)
#dendograms
plot(hClustering)
```

>Headmap
```{r}
dataFrame <- data.frame(x = x, y = y)
set.seed(143)
dataMatrix <- as.matrix(dataFrame)[sample(1:12),]
heatmap(dataMatrix)
```

## K-Means Clustering & Dimension Reduction

### K-means clustering

How do we define *close*?

#### Most important step
  + garbage in -> garbage out

#### Distance or similarity
  + Continuous - euclidean distance
  + Continuous - correlation similarity
  + Binary - manhattan distance
  + Pick a distance / similarity that makes sense for your problem

#### A partitioning approach
  + Fix a number of clusters
  + Get "centroids" of each cluster
  + Assign things to closest centroid
  + Recalculate centroids
  
#### Requires
  + A defined distance metric
  + A number of clusters
  + An initial guess as of cluster centroids
  
#### Produces
  + Final estimate of cluster centroids
  + An assignment of each point of clusters
  
  
> K-means Clustering - Example

```{r}
set.seed(1234)
par(mar=c(0,0,0,0))
x <- rnorm(12, mean=rep(1:3, each=4), sd=0.2)
y <- rnorm(12, mean=rep(1,2,1, each=4), sd=0.2)
plot(x, y, col='blue', pch=19, cex=2)
text(x + 0.05, y + 0.05, labels=as.character(1:12))
```

#### K-means()

* Important paraments: x, centers, iter.max, nstart

```{r}
dataFrame <- data.frame(x, y)
kmeansObj <- kmeans(dataFrame, centers=3)
names(kmeansObj)
kmeansObj$cluster
```
```{r}
#K-means()
par(mar = rep(0.2, 4))
plot(x, y, col = kmeansObj$cluster, pch=19, cex = 2)
points(kmeansObj$centers, col=1:3, pch=3, cex=2)
```

#### Heatmaps

```{r}
set.seed(1234)
dataMatrix <- as.matrix(dataFrame)[sample(1:12),]
kmeansObj2 <- kmeans(dataMatrix, centers= 3)
par(mfrow = c(1,2), mar = c(1,4,0.1,0.1))
image(t(dataMatrix)[, nrow(dataMatrix):1], yaxt = "n")
image(t(dataMatrix)[, order(kmeansObj$cluster)], yaxt = "n")
```

### Dimension Reduction
> Let's generates a Matrix data and plot as image

```{r}
# A Random Matrix Plot
set.seed(1234)
par(mar=rep(0.2, 4))
dataMatrix <- matrix(rnorm(400), nrow=40)
image(1:10, 1:40, t(dataMatrix)[, nrow(dataMatrix):1])
```

> Cluster the data

```{r}
par(mar = rep(0.2, 4))
heatmap(dataMatrix)

# Observe there is no interesting pattern to the data

```

> What if we add a pattern ?

```{r}
set.seed(678910)
for(i in 1:40){
  # flip a coin
  coinFlip <- rbinom(1, size=1, prob=0.5)
  #if the coin is head adda common pattern to that row
  if (coinFlip){
    dataMatrix[i, ] <- dataMatrix[i, ] + rep(c(0,3), each=5)
  }
}
par(mar = rep(0.2, 4))
image(1:10, 1:40, t(dataMatrix)[, nrow(dataMatrix):1])

```

> Run the heatmap again

```{r}
par(mar = rep(0.2, 4))
heatmap(dataMatrix)
```

> Patterns in rows and columns

```{r}
hh <- hclust(dist(dataMatrix))
dataMatrixOrdered <- dataMatrix[hh$order,]
par(mfrow=c(1,3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(rowMeans(dataMatrixOrdered), 40:1,xlab="Row Mean", ylab="Row", pch=19)
plot(colMeans(dataMatrixOrdered),xlab="Column", ylab="Column Mean", pch=19)
```

***

### **Related Problems**

You have multivariate variables *X1*,....,*Xn* so *X1* = (*X^11^*,.......*X^1m^*)

* Find a new set of multivariate variables that are uncorrelated and explain as much variance as possible.
* If you put all the variables in one matrix, find the best matrix created with fewer variables (lower rank) that explains the original data

The first goal is **statistical** and the second goal is **data compression**.

#### **Related Solution**

**SVD - Singular Value Decomposition**

If *X* is a matrix with each variable in a column and each observation in a row then the SVD is a "matrix decomposition"

                    X = *UVD^T^*
                    
where the columns of *U* are the orthogonal (left singular vectors), the columns of *V* are orthogonal (right singular vector) and *D* is a diagonal vector matrix (singular values)

** PCA - Principal components analysis **

The principal components are equal to the right singular values if you first scale (subtract the mean, divide by the standard deviation) of the variables.


### Components of the SVD - *u* and *v*

```{r}
svd1 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1,3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(svd1$u[,1], 40:1, xlab="Row", ylab="First Left Singular vector", pch=19)
plot(svd1$v[,1], xlab="Column", ylab="First Right Singular vector", pch=19)

```

### Components of the SVD - Variance explained

```{r}
par(mfrow=c(1,2))
plot(svd1$d, xlab="Column",ylab="Singular Value", pch=19)
plot(svd1$d^2/sum(svd1$d^2), xlab="Column", ylab="Prop. of Variance explained", pch=19)

```

### Relationship to principal components

```{r}
svd1 <- svd(scale(dataMatrixOrdered))
pca1 <- prcomp(dataMatrixOrdered, scale = TRUE)
plot(pca1$rotation[,1], svd1$v[,1], pch=19, xlab="Principal Component 1", ylab="Right Singular Vector 1")
abline(c(0,1))
```

### Components of the SVD - variance explained

```{r}
constantMatix <- dataMatrixOrdered * 0
for (i in 1 : dim(dataMatrixOrdered)[1]){
  constantMatix[i,] <- rep(c(0,1),each=5)
  }
  
svd1 <- svd(constantMatix)
par(mfrow=c(1,3))

image(t(constantMatix)[,nrow(constantMatix):1])
plot(svd1$d, xlab="Column", ylab="Singular Value", pch=19)
plot(svd1$d^2/sum(svd1$d^2), xlab="Column", ylab="Prop. of variance explained", pch=19)
```

### What if we add a second pattern?

```{r}
set.seed(678910)
for (i in 1:40){
  #flipe a coin
  coinflip1 <- rbinom(1, size=1, prob=0.5)
  coinflip2 <- rbinom(1, size=1, prob=0.5)
  
  if (coinflip1){
    dataMatrix[i,] <- dataMatrix[i,] + rep(c(0,5), each=5)
  }
  if (coinflip1){
    dataMatrix[i,] <- dataMatrix[i,] + rep(c(0,5), each=5)
  }
}

hh <- hclust(dist(dataMatrix))
dataMatrixOrdered <- dataMatrix[hh$order,]
```

> Singular Value decomposition - true patterns

```{r}
## Let's plot it

svd2 <- svd(scale(dataMatrixOrdered))
par(mfrow=c(1,3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(rep(c(0,1), each=5),pch=19, xlab="Column", ylab="Pattern 1")
plot(rep(c(0,1), 5),pch=19, xlab="Column", ylab="Pattern 2")
```

> v and patterns of varinace in rows

```{r}
## Let's plot it

svd2 <- svd(scale(dataMatrixOrdered))
par(mfrow=c(1,3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(svd2$v[, 1],pch=19, xlab="Column", ylab="First right singular vector")
plot(svd2$v[, 2],pch=19, xlab="Column", ylab="Second right singular vector")
```

> d and variance explained

```{r}
svd1 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1,2))
plot(svd1$d, xlab="Column", ylab= "Singular Value", pch=19)
plot(svd1$d^2/sum(svd1$d^2), xlab="Column", ylab= "Percentage of variance explained", pch=19)

```

## Missing Values

Always deal with missing values before running *SVD* or *PCA* analysis.

### Imputing {impute}

```{r}
library(impute) #Available from https://bioconductor.org #biocLite
dataMatrix2 <- dataMatrixOrdered
dataMatrix2[sample(1:100, size=40, replace=FALSE)] <- NA
dataMatrix2 <- impute.knn(dataMatrix2)$data
svd1 <- svd(scale(dataMatrixOrdered))
svd2 <- svd(scale(dataMatrix2))

par(mfrow = c(1,2))
plot(svd1$v[,1], pch=19)
plot(svd2$v[,1], pch=19)
```

# Plotting and Color in R

**Color Utilities in R**  

* The **grDevices** pacckage has two functions
  - colorRamp
  - colorRampPalette
  
* These function take palettes of colors and help to interpolate between the colors

* The function colors() lists the names of colors you can use in any plotting function

* colorRamp: Take a palette of colors and return a function that takes a value between 0 annd 1, indicating the extremes of the color palette (e.g. see the 'gray' function)

* colorRampPalette: Take a palette of colors and return a funcction that takes integer arguments and returns a vector of colors interpolating the palette (like head.colors or topo.colors)

> Example

```{r}
pal <- colorRamp(c("red","blue"))
pal(0)

pal(1)

pal(0.5)
```

> Another Example

```{r}
pal(seq(0,1,len=10))
```

> **colorRampPalette** on the other hand will return the hex color value

```{r}
pal <- colorRampPalette(c("red","yellow"))

pal(2)
pal(10)
```

## RColorBrewer Package

* One package on CRAN that contains interesting/useful color palettes

* There are 3 types of palettes

 - Sequential   - They go in specific order
 - Diverging    - Used to show ranges (e.g., negative to positive)
 - Qualitative  - Used for classification
 
* Palette information can be used in conjunction with the colorRamp() and colorRampPalette()

> **Example**

```{r}
library(RColorBrewer)
cols <- brewer.pal(3, "BuGn")
cols

pals <- colorRampPalette(cols)
image(volcano, col=pals(20))
```

> Another Example **The smoothScatter function**

```{r}
x <- rnorm(10000)
y <- rnorm(10000)
smoothScatter(x,y)
```

### Some other plotting notes

* The rgb function can be used to produce any color via red, greeb and blue proportions

* Color transparency can be added via the alpha parameter to rgb

* the **colorspace** package can be used for a different control over colors








