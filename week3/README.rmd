---
title: "README"
author: "Chandrasekar Ganesan"
date: "March 2, 2017"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Hierarchical Clustering - Example
```{r}
set.seed(1234)
par(mar=c(0,0,0,0))
x <- rnorm(12, mean=rep(1:3, each=4), sd=0.2)
y <- rnorm(12, mean=rep(1,2,1, each=4), sd=0.2)
plot(x, y, col='blue', pch=19, cex=2)
text(x + 0.05, y + 0.05, labels(as.character(1:12)))
```

>Hierarchical Clusterring - Dist
```{r}
dataFrame <- data.frame(x = x, y = y)
dist(dataFrame)
```

>Hierarchical Clustering- hclust
```{r}
distxy <- dist(dataFrame)
hClustering <- hclust(distxy)
#dendograms
plot(hClustering)
```

>Headmap
```{r}
dataFrame <- data.frame(x = x, y = y)
set.seed(143)
dataMatrix <- as.matrix(dataFrame)[sample(1:12),]
heatmap(dataMatrix)
```

## K-Means Clustering & Dimension Reduction

### K-means clustering

How do we define *close*?

#### Most important step
  + garbage in -> garbage out

#### Distance or similarity
  + Continuous - euclidean distance
  + Continuous - correlation similarity
  + Binary - manhattan distance
  + Pick a distance / similarity that makes sense for your problem

#### A partitioning approach
  + Fix a number of clusters
  + Get "centroids" of each cluster
  + Assign things to closest centroid
  + Recalculate centroids
  
#### Requires
  + A defined distance metric
  + A number of clusters
  + An initial guess as of cluster centroids
  
#### Produces
  + Final estimate of cluster centroids
  + An assignment of each point of clusters
  
  
> K-means Clustering - Example

```{r}
set.seed(1234)
par(mar=c(0,0,0,0))
x <- rnorm(12, mean=rep(1:3, each=4), sd=0.2)
y <- rnorm(12, mean=rep(1,2,1, each=4), sd=0.2)
plot(x, y, col='blue', pch=19, cex=2)
text(x + 0.05, y + 0.05, labels=as.character(1:12))
```

#### K-means()

* Important paraments: x, centers, iter.max, nstart

```{r}
dataFrame <- data.frame(x, y)
kmeansObj <- kmeans(dataFrame, centers=3)
names(kmeansObj)
kmeansObj$cluster
```
```{r}
#K-means()
par(mar = rep(0.2, 4))
plot(x, y, col = kmeansObj$cluster, pch=19, cex = 2)
points(kmeansObj$centers, col=1:3, pch=3, cex=2)
```

#### Heatmaps

```{r}
set.seed(1234)
dataMatrix <- as.matrix(dataFrame)[sample(1:12),]
kmeansObj2 <- kmeans(dataMatrix, centers= 3)
par(mfrow = c(1,2), mar = c(1,4,0.1,0.1))
image(t(dataMatrix)[, nrow(dataMatrix):1], yaxt = "n")
image(t(dataMatrix)[, order(kmeansObj$cluster)], yaxt = "n")
```

### Dimension Reduction
> Let's generates a Matrix data and plot as image

```{r}
# A Random Matrix Plot
set.seed(1234)
par(mar=rep(0.2, 4))
dataMatrix <- matrix(rnorm(400), nrow=40)
image(1:10, 1:40, t(dataMatrix)[, nrow(dataMatrix):1])
```

> Cluster the data

```{r}
par(mar = rep(0.2, 4))
heatmap(dataMatrix)

# Observe there is no interesting pattern to the data

```

> What if we add a pattern ?

```{r}
set.seed(678910)
for(i in 1:40){
  # flip a coin
  coinFlip <- rbinom(1, size=1, prob=0.5)
  #if the coin is head adda common pattern to that row
  if (coinFlip){
    dataMatrix[i, ] <- dataMatrix[i, ] + rep(c(0,3), each=5)
  }
}
par(mar = rep(0.2, 4))
image(1:10, 1:40, t(dataMatrix)[, nrow(dataMatrix):1])

```

> Run the heatmap again

```{r}
par(mar = rep(0.2, 4))
heatmap(dataMatrix)
```

> Patterns in rows and columns

```{r}
hh <- hclust(dist(dataMatrix))
dataMatrixOrdered <- dataMatrix[hh$order,]
par(mfrow=c(1,3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(rowMeans(dataMatrixOrdered), 40:1,xlab="Row Mean", ylab="Row", pch=19)
plot(colMeans(dataMatrixOrdered),xlab="Column", ylab="Column Mean", pch=19)
```

***

### **Related Problems**

You have multivariate variables *X1*,....,*Xn* so *X1* = (*X^11^*,.......*X^1m^*)

* Find a new set of multivariate variables that are uncorrelated and explain as much variance as possible.
* If you put all the variables in one matrix, find the best matrix created with fewer variables (lower rank) that explains the original data

The first goal is **statistical** and the second goal is **data compression**.

#### **Related Solution**

**SVD - Singular Value Decomposition**

If *X* is a matrix with each variable in a column and each observation in a row then the SVD is a "matrix decomposition"

                    X = *UVD^T^*
                    
where the columns of *U* are the orthogonal (left singular vectors), the columns of *V* are orthogonal (right singular vector) and *D* is a diagonal vector matrix (singular values)

** PCA - Principal components analysis **

The principal components are equal to the right singular values if you first scale (subtract the mean, divide by the standard deviation) of the variables.


### Components of the SVD - *u* and *v*

```{r}
svd1 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1,3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(svd1$u[,1], 40:1, xlab="Row", ylab="First Left Singular vector", pch=19)
plot(svd1$v[,1], xlab="Column", ylab="First Right Singular vector", pch=19)

```

